import sys, chromadb, requests
from chromadb.utils import embedding_functions

def check_ollama_service():
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except requests.exceptions.ConnectionError:
        return False
    except requests.exceptions.Timeout:
        return False
    except Exception:
        return False

def print_error(message):
    print(f"\nâŒ é”™è¯¯: {message}")
    sys.exit(1)

if not check_ollama_service():
    print_error("Ollama æœåŠ¡ä¸å¯ç”¨\n\nè¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ:\n  1. è¿è¡Œ 'ollama serve' å¯åŠ¨æœåŠ¡\n  2. æˆ–è®¿é—® https://ollama.ai å®‰è£… Ollama")

try:
    ollama_ef = embedding_functions.OllamaEmbeddingFunction(
        url="http://localhost:11434/api/embeddings",
        model_name="nomic-embed-text",
    )
    client = chromadb.PersistentClient(path="./vector_db")
    collection = client.get_or_create_collection("personal_knowledge", embedding_function=ollama_ef)
except Exception as e:
    print_error(f"åˆå§‹åŒ–å‘é‡æ•°æ®åº“å¤±è´¥: {e}")

if len(sys.argv) < 2:
    print_error("è¯·æä¾›æœç´¢å…³é”®è¯\n\nç”¨æ³•: python search.py <æœç´¢å…³é”®è¯>")

query = " ".join(sys.argv[1:])
print(f"\nğŸ” æœç´¢: {query}\n")

try:
    results = collection.query(query_texts=[query], n_results=5)
    
    if not results['documents'][0]:
        print("æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        sys.exit(0)
    
    for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
        print(f"[{i+1}] æ¥æº: {meta['source']}")
        print(doc[:300] + "...")
except Exception as e:
    print_error(f"æœç´¢å¤±è´¥: {e}")
