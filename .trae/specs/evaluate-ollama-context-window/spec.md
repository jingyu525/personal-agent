# Ollama 模型上下文窗口评估 Spec

## Why
评估当前 ollama 环境中可用模型的上下文窗口大小，判断是否满足日常开发、对话和代码辅助等需求，为模型选择提供参考依据。

## What Changes
- 分析已安装模型的上下文窗口规格
- 评估不同场景下的上下文需求
- 提供模型选择建议和优化方案

## Impact
- 影响范围：模型选择策略、对话历史管理、代码辅助能力
- 相关系统：personal-agent 项目中的模型调用逻辑

## ADDED Requirements

### Requirement: 模型上下文窗口分析
系统 SHALL 提供对已安装 ollama 模型的上下文窗口能力评估。

#### Scenario: 获取模型上下文信息
- **WHEN** 用户请求评估模型上下文窗口
- **THEN** 系统应返回每个模型的默认上下文窗口大小和最大支持上下文

#### Scenario: 场景化需求评估
- **WHEN** 分析不同使用场景
- **THEN** 系统应提供各场景的上下文需求量级和推荐模型

## 模型上下文窗口规格

### 已安装模型列表
1. **qwen2.5:7b** - 4.7 GB
   - 默认上下文：2048 tokens
   - 最大上下文：131,072 tokens (128K)
   - 特点：支持超长上下文，适合长文档处理

2. **qwen2.5:14b** - 9.0 GB
   - 默认上下文：2048 tokens
   - 最大上下文：131,072 tokens (128K)
   - 特点：更强的推理能力，支持超长上下文

3. **qwen2.5-coder:7b** - 4.7 GB
   - 默认上下文：2048 tokens
   - 最大上下文：131,072 tokens (128K)
   - 特点：代码专用模型，支持代码生成、推理和修复

4. **llama3.2:3b** - 2.0 GB
   - 默认上下文：4096 tokens (4K)
   - 最大上下文：131,072 tokens (128K)
   - 特点：轻量级，适合快速响应

5. **deepseek-r1:latest** - 5.2 GB
   - 默认上下文：4096 tokens (4K)
   - 最大上下文：131,072 tokens (128K)
   - 特点：推理能力强，适合复杂推理任务

6. **deepseek-ocr:latest** - 6.7 GB
   - 默认上下文：4096 tokens (4K)
   - 最大上下文：131,072 tokens (128K)
   - 特点：OCR专用模型，支持图像文字识别

7. **nomic-embed-text:latest** - 274 MB
   - 用途：文本嵌入模型，不涉及对话上下文

## 场景化需求评估

### 1. 简单对话场景
- **需求量级**：500-2000 tokens
- **典型情况**：日常问答、简单咨询
- **评估**：✅ 所有模型默认配置均满足

### 2. 代码辅助场景
- **需求量级**：2000-8000 tokens
- **典型情况**：
  - 单文件代码审查：2000-4000 tokens
  - 多文件上下文：4000-8000 tokens
  - 项目级理解：8000-16000 tokens
- **评估**：
  - ✅ qwen2.5 系列：默认 2K 略显不足，建议调至 8K+
  - ✅ llama3.2：默认 4K 基本满足单文件场景
  - ✅ deepseek-r1：默认 4K 基本满足单文件场景

### 3. 长文档处理场景
- **需求量级**：10,000-50,000 tokens
- **典型情况**：
  - 技术文档分析：10K-20K tokens
  - 论文阅读总结：15K-30K tokens
  - 书籍章节处理：30K-50K tokens
- **评估**：
  - ✅ qwen2.5 系列：支持 128K，完全满足
  - ✅ llama3.2：支持 128K，完全满足
  - ⚠️ deepseek-r1：支持 64K，满足大部分场景

### 4. 多轮对话场景
- **需求量级**：累积增长，每轮 200-1000 tokens
- **典型情况**：
  - 10轮对话：2000-10000 tokens
  - 20轮对话：4000-20000 tokens
  - 50轮对话：10000-50000 tokens
- **评估**：
  - ⚠️ 默认配置下，长对话可能丢失早期上下文
  - ✅ 需要调整 num_ctx 参数或实现对话压缩

### 5. Agent 任务执行场景
- **需求量级**：动态变化，可能 5000-30000 tokens
- **典型情况**：
  - 工具调用链：5000-15000 tokens
  - 复杂推理任务：10000-30000 tokens
  - 代码生成与修改：8000-20000 tokens
- **评估**：
  - ✅ qwen2.5 系列：完全满足
  - ✅ llama3.2：完全满足
  - ✅ deepseek-r1：满足大部分场景

## 总体评估结论

### ✅ 满足日常需求的场景
1. **简单对话**：所有模型默认配置均满足
2. **单文件代码辅助**：llama3.2 和 deepseek-r1 默认满足，qwen2.5 需调参
3. **长文档处理**：所有模型都支持扩展到足够大的上下文
4. **Agent 任务**：所有模型在适当配置下均满足

### ⚠️ 需要优化的场景
1. **多轮长对话**：需要实现对话历史管理策略
   - 方案1：调整 num_ctx 参数增大上下文窗口
   - 方案2：实现对话摘要和压缩机制
   - 方案3：使用滑动窗口保留最近 N 轮对话

2. **大项目代码理解**：需要智能上下文选择
   - 方案：实现 RAG 或智能文件选择机制

### 💡 推荐配置建议

#### 日常使用推荐
- **快速响应**：llama3.2:3b (num_ctx=8192)
- **平衡选择**：qwen2.5:7b (num_ctx=16384)
- **代码专用**：qwen2.5-coder:7b (num_ctx=16384)
- **复杂任务**：qwen2.5:14b 或 deepseek-r1 (num_ctx=32768)

#### 特定场景推荐
- **代码辅助**：qwen2.5-coder:7b + num_ctx=16384
- **长文档分析**：qwen2.5:14b + num_ctx=65536
- **推理任务**：deepseek-r1 + num_ctx=32768
- **OCR任务**：deepseek-ocr:latest + num_ctx=16384

## 实施建议

### 短期优化
1. 在 personal-agent 中添加模型上下文配置参数
2. 实现基于场景的自动上下文窗口调整
3. 添加对话历史管理机制

### 长期优化
1. 实现 RAG 系统处理大规模代码库
2. 开发智能上下文选择策略
3. 建立模型性能监控和自动切换机制
